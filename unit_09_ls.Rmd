---
title: "Unit 9 Live Session: Large-Sample Regression Theory (OLS Inference)"
output: 'pdf_document'  
classoption: landscape
fontsize: 12pt
---

```{r, message=FALSE, echo=FALSE}
library(tidyverse)
library(sandwich)
library(lmtest)

theme_set(theme_minimal())
```

\newcommand{\E}{\text{E}}
\newcommand{\Cov}{\mathbb{\text{Cov}}}
\newcommand{\V}{\text{V}}

# Linear Regression {data-background='https://imgs.xkcd.com/comics/linear_regression.png' data-background-size="900px"}

![Linear regression](./images/linear_regression.png){width=50%}

\newpage

## Class Announcements

1. Congratulations on finishing your first lab!
2. The next (and the last) lab is coming up in two weeks. 
2. Homework 09 has been released, and it's due next Tuesday.

# Roadmap

## Rear-View Mirror

- Statisticians create a population model to represent the world.
- Sometimes, the model includes an "outcome" random variable $Y$ and "input" random variables $X_1, X_2,...,X_k$.
- The joint distribution of $Y$ and $X_1, X_2,...,X_k$ is complicated.
- The best linear predictor (BLP) is the canonical way to summarize the relationship.
- OLS provides a point estimate of the BLP

## Today

- Robust Standard Error: quantify the uncertainty of OLS coefficients
- Hypothesis testing with OLS coefficients
- Bootstrapping

## Looking Ahead

- Regression is a foundational tool that can be applied to different contexts
- The process of building a regression model looks different, depending on whether the goal is prediction, description, or explanation.

\newpage 

# Uncertainty in OLS
## Discussion Questions

- List as many differences between the BLP and the OLS line as you can.
- In the following regression table, explain in your own words what the standard error in parentheses means.


|               | outcome: sleep hours |
|---------------|----------------------|
| mg. melatonin |    0.52              |
|               |   (0.31)             |
   

\newpage
## Understanding Uncertainty

Imagine three different regression models, each of the following form:

$$Y = 0 + 0 \cdot X + \epsilon$$

The only difference is in the error term. The conditional distribution is given by:

Model | Distribution of $\epsilon$ cond. on $X$
------|----------------------------------------
 A    | Uniform on $ [-.5, +.5]$
 B    | Uniform on $[ - |X|, |X| ]$
 C    | Uniform on $[ -1 + |X|, 1- |X| ]$
 
A is what we call a homoskedastic distribution. B and C are what we call heteroskedastic. Below, we define R functions that simulate draws from these three distributions.
 
```{r}

rA <- function(n){
  x = runif(n, min=-1, max = 1)
  epsilon = runif(n, min=-.5, max=.5)
  y= 0 + epsilon
  return( data.frame(x=x,y=y) )
}

rB <- function(n){
  x = runif(n, min=-1, max = 1)
  epsilon = runif(n, min=- abs(x), max=abs(x))
  y= 0 + epsilon
  return( data.frame(x=x,y=y) )
}

rC <- function(n){
  x = runif(n, min=-1, max = 1)
  epsilon = runif(n, min= -1 + abs(x), max=1 - abs(x))
  y= 0 + epsilon
  return( data.frame(x=x,y=y) )
}
```

```{r}
data <- rbind( data.frame( rA(200), label = 'A'),
               data.frame( rB(200), label = 'B'),
               data.frame( rC(200), label = 'C'))
data %>% ggplot(aes(x=x, y=y)) + geom_point() + xlim(-2,2) + ylim(-1,1) + facet_grid(rows=vars(label)) + ggtitle('Samples Drawn from Three Distributions')
```

\newpage
**Q1** The following code draws a sample from distribution A, fits a regression line, and plots it. Run it a few times to see what happens. Now explain how you would visually estimate the standard error of the slope coefficient. Why is this standard error important?

```{r}
data = rA(10)
data %>% ggplot(aes(x=x, y=y)) + geom_point() + geom_smooth(method='lm', se=FALSE) + xlim(-2,2) + ylim(-1,1) + ggtitle('Regression Fit to Distribution A')
```

\newpage
**Q2** You have a sample from each distribution, A, B, and C and you fit a regression of Y on X. Which will have the highest standard error for the slope coefficient? Which will have the lowest standard error? Why? (You may want to try experimenting with the function defined above)

### R Cheat-Sheet

Some R commands that might help with the next question:

```{r}
replicate(3, rnorm(1))
```

```{r}
experiment <- function(){
  return(1)
}
experiment()
```
```{r}
lm(y ~ x, data = rA(10))$coef
```



**Q3** For distribution A, perform a simulated experiment. Draw a large number of samples, and for each sample fit a linear regression. Store the slope coefficient from each regression in a vector. Finally, compute the standard deviation for the slope coefficients.

Repeat this process for distributions B and C. Do the results match your intuition?


\newpage

## More About Standard Errors

Under the relatively stricter assumptions of constant error variance, the variance of a slope coefficient is given by

$$
  \V(\hat{\beta_j}) = \frac{\sigma^2}{SST_j (1-R_j^2)}
$$

A similar formulation is given in *FOAS* as definition 4.2.3, 

$$
  \hat{V}_{C}[\hat{\beta}] = \hat{\sigma}^2 \left( X^{T} X \right)^{-1} \rightsquigarrow \frac{\hat{\sigma}^{2}}{\left( X^{T}X\right)}
$$

Explain why each term makes the variance higher or lower:

- $\sigma^2$ is the variance of the error $\epsilon$
- $SST_j$ is (unscaled) variance of $X_j$
- $R_j^2$ is $R^2$ for a regression of $X_j$ on the other $X$'s


\newpage
# Coding Activity
## R Cheat Sheet

Suppose `x` and `y` are variables in dataframe `d`.

To fit an ols regression of Y on X:

    mod <- lm(y ~ x, data = d)

To access **coefficients** from the model object:

    mod$coefficients
    or coef(mod)
    
To get a robust covariance matrix

    vcovHC(mod)
    
To run t-tests for all coefficients using robust standard errors

    coeftest(mod, vcov = vcovHC)
    
To F-test for a joint restriction
    
    waldtest(model_simple, model_full, vcov = vcovHC(model_full, type = "HC0"))

  
\newpage
## Real Estate in Boston 

The file `hprice1.RData` contains 88 observations of homes in the Boston area, taken from the real estate pages of the Boston Globe during 1990.  This data was provided by Wooldridge.

```{r}
load('hprice1.RData') # provides 3 objects 
```

Last week, we fit a regression of price on square feet. 

```{r}
model_one <- lm(price ~ sqrft, data = data)
model_one
```

1. Estimate a new model (and save it into another object) that includes the size of the lot and whether the house is a colonial. This will estimate the model: 

\[ 
  price = \beta_{0} + \beta_{1} sqrft + \beta_{2} lotsize + \beta_{3} colonial? + e
\] 

- *BUT BEFORE YOU DO*, make a prediction: What do you think is going to happen to the coefficient that relates square footage and price? 

  - Will the coefficient increase, decrease, or stay the same? 
  - Will the *uncertainty* about the coefficient increase, decrease, or stay the same? 
  - Conduct an F-test that evaluates whether the model *as a whole* does better when the coefficients on `colonial` and `lotsize` are allowed to            estimate freely, or instead are restricted to be zero (i.e. $\beta_{2} = \beta_{3} = 0$. 


2. Use the function `vcovHC` from the `sandwich` package to estimate (a) the the heteroskedastic consistent (i.e. "robust") variance covariance matrix; and (b) the robust standard errors for the intercept and slope of this regression. Recall, what is the relationship between the VCOV and SE in a regression? 

```{r}

```

3. Perform a hypothesis test to check whether the population relationship between `sqrft` and `price` is zero. Use `coeftest()` with the robust standard errors computed above. 

```{r}

```

4. Use the robust standard error and `qt` to compute a 95% confidence interval for the coefficient `sqrft` in the second model that you estimated. $price = \beta_{0} + \beta_{1} sqrft + \beta_{2} lotsize + \beta_{3} colonial$. 

\newpage

5. **Bootstrap.** The book *very* quickly talks about bootstrapping which is the process of sampling *with replacement* and fitting a model. The idea behind the bootstrap is that since the data is generated via an iid sample from the population, that you can simulate re-running your analysis by drawing repeated samples from the data that you have. 

Below is code that will conduct a boostrapping estimator of the uncertainty of the `sqrft` variable when `lotsize` and `colonial` are included in the model. 

```{r conduct a boostrap}
bootstrap_sqft <- function(d = data, number_of_bootstraps = 1000) { 
  number_of_rows <- nrow(d)

    coef_sqft <- rep(NA, number_of_bootstraps)

    for(i in 1:number_of_bootstraps) { 
      bootstrap_data <- d[sample(x=1:number_of_rows, size=number_of_rows, replace=TRUE), ]  
      estimated_model <- lm(price ~ sqrft + lotsize + colonial, data = bootstrap_data)
      coef_sqft[i]    <- coef(estimated_model)['sqrft']
    }
  return(coef_sqft)
  }
```

```{r}
bootstrap_result <- bootstrap_sqft(number_of_bootstraps = 1000)
```

With this, it is possible to plot the distribution of these regression coefficients: 

```{r}
ggplot() + 
  aes(x = bootstrap_result) + 
  geom_histogram() + 
  labs(
    x = 'Estimated Coefficient', 
    y = 'Count', 
    title = 'Bootstrap coefficients for square footage'
  )
```

Compute the standard deviation of the bootstrapped regression coefficients.  How does this compare to the robust standard errors you computed above?